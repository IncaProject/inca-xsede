- Operations and gig-pack activities:  Participated in and occasionally led
  bi-weekly operations working group calls.  Ongoing configuration and
troubleshooting work with system administrators for CTSSv4 tests and TG wide
services like MDS.  Created new tests and status pages for gig-pack and CTSSv4
status pages.

- Real-time monitoring example:  Coordinated and organized an example of
  real-time monitoring using Inca on SDSC resources.  We worked with SDSC user
services, Stu Martin, LEAD developers, and NCSA (Doru Marcusiu and Jay Alameda)
to identify an initial set of services to monitor and a set of tests to test
them.  In order to keep the testing load on resources low, we deployed a range
of tests from light weight methods that can be run very frequently to detect
hard failures (i.e., process not running) to more heavy weight methods that can
be run less frequently to detect more finer grained problems (i.e., stage of
files for a job fail).  We started first with GRAM monitoring (pre-WS and WS)
and deployed a set of four tests for each service.  When a failure is detected,
an email notification is sent to SDSC user services to verify the problem and
work with operations to fix it.  The real-time monitoring example has been
running at SDSC for two months now and the tests have been vetted to ensure the
failures detected correspond to real failures.  Next steps include deploying
the real-time monitoring to a couple more friendly sites and add additional
services to be monitored.  The web page link to the real-time monitoring page
can be found at:

http://sapa.sdsc.edu:8080/inca/xslt.jsp?xsl=swStack.xsl&resourceID=real-time&suiteName=real-time&xmlFile=real-time.xml

- Historical graphs:  Created graphs that display the history of one or more
  individual Inca tests.  A graph can be viewed from the details page of an
individual test or from the "HISTORICAL DATA" link in the Inca
navigation bar, which allows multiple tests to be displayed on a single graph.
A test success is displayed as 1 and a test failure as 0 and the history is
plotted on the graph as a line.  The number of tests, the number of failed
reports, the number of passed reports, and the percentage of tests that have
passed are displayed below the graph.  A second graph with error message
frequency is also displayed if a test has errors.  The error messages and
frequency of each message are also listed in a table below the graph.
Furthermore, a form allows for customization of the graph appearance such as
title, x-axis label, etc.

- Custom views (in progress):  Helped to design and implement a new feature
  targeted to portal developers or advanced users to create customized views of
the Inca data.  A custom view can display the status of a select number of
TeraGrid resources and/or a select number of tests used by a particular
application.  Custom views are created using a web page interface that allows a
user to select the data they want to display.  After the data is selected, a
stylesheet can be applied to the data to display the custom view.
